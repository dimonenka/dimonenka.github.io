<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Dima Ivanov </title> <meta name="author" content="Dima Ivanov"> <meta name="description" content=""> <meta name="keywords" content="Dima, Dmitrii, Dmitry, Ivanov, Academic, Personal, Webpage"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%97%BF&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://dimonenka.github.io/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/papers/">Papers </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Dima</span> Ivanov </h1> <p class="desc" style="font-weight: bold;">Aligned with AI</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," sizes="(min-width: 800px) 231.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/prof_pic.jpg?a569b7aaaa85ac38df9afd9925596b0c" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <div style="background-color: #330845; text-align: center; padding: 5px; margin: 5pt 0;"> <a href="/assets/pdf/CV.pdf" target="_blank" style="color: white; font-size: 20px; text-decoration: none;"> üìñ My CV </a> </div> </div> </div> <div class="clearfix"> <p>I am a research scientist in AI and Game Theory, including Multi-Agent Reinforcement Learning with a focus on incentive alignment and cooperation, as well as Automated Mechanism Design.</p> <p>Currently, I am a postdoc in the <a href="https://dds.technion.ac.il/" target="_blank" rel="noopener noreferrer">Data and Decision Sciences</a> faculty at Technion in Haifa, Israel. Prior to this, I pursued my doctoral studies at the Higher School of Economics in St. Petersburg, Russia, where I am expected to obtain a PhD in Artificial Intelligence and Machine Learning later in 2024. I was doing research in the <a href="https://game.hse.ru/en/" target="_blank" rel="noopener noreferrer">Game Theory and Decision Making</a> lab at the Higher School of Economics, as well as in the Reinforcement Learning unit of JetBrains Research.</p> <p>Contact me via divanov.ml@gmail.com</p> <h3 style="color: inherit">Research Interests</h3> <p>My research lies at the intersection of AI and Game Theory, focusing on two directions.</p> <p>One direction is to apply methods of Machine Learning to economic problems. As a notable example, automated design of economic mechanisms employs data-driven ML approaches to discover (approximately) optimal mechanisms with desirable properties, moving beyond traditional reliance on theorems and proofs. In this area, I have contributed to auction and contract design through deep learning.</p> <p>The other direction is to take an economic perspective on AI problems. This implies treating AI agents as entities with inherent incentives and preferences, which we cannot modify directly, but can influence externally through designed mechanisms ‚Äì rules governing their interactions with the world, each other, and us. This concept I study through the lens of Multi-Agent Reinforcement Learning.</p> <p>Despite the rising concerns in academy and society alike, I do not believe that Artificial General Intelligence is imminent. Still, by the principles of <a href="https://arbital.com/p/orthogonality/" target="_blank" rel="noopener noreferrer">orthogonality</a> and <a href="https://arbital.com/p/instrumental_convergence/" target="_blank" rel="noopener noreferrer">instrumental convergence</a>, it is inevitable, and ensuring its safety and alignment with our values (whatever that means) is vital. Given that Game Theory, Mechanism Design, and related fields explicitly concern alignment of incentives, integrating these fields into AI research is a necessary precursor towards safe AGI.</p> </div> <h2 style="color: inherit">News</h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Feb 13, 2024</th> <td> Still kicking! </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Selected Papers</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div id="ivanov2024personalized" class="col-sm-10"> <div class="title">Personalized Reinforcement Learning with a Budget of Policies</div> <div class="author"> <em>Dmitry Ivanov</em> ,¬† and¬† <a href="https://sites.google.com/site/omerbp/" rel="external nofollow noopener" target="_blank">Omer Ben-Porat</a> </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2401.06514" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Personalization in machine learning (ML) tailors models‚Äô decisions to the individual characteristics of users. While this approach has seen success in areas like recommender systems, its expansion into high-stakes fields such as healthcare and autonomous driving is hindered by the extensive regulatory approval processes involved. To address this challenge, we propose a novel framework termed represented Markov Decision Processes (r-MDPs) that is designed to balance the need for personalization with the regulatory constraints. In an r-MDP, we cater to a diverse user population, each with unique preferences, through interaction with a small set of representative policies. Our objective is twofold: efficiently match each user to an appropriate representative policy and simultaneously optimize these policies to maximize overall social welfare. We develop two deep reinforcement learning algorithms that efficiently solve r-MDPs. These algorithms draw inspiration from the principles of classic K-means clustering and are underpinned by robust theoretical foundations. Our empirical investigations, conducted across a variety of simulated environments, showcase the algorithms‚Äô ability to facilitate meaningful personalization even under constrained policy budgets. Furthermore, they demonstrate scalability, efficiently adapting to larger policy budgets.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ivanov2024personalized</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Personalized Reinforcement Learning with a Budget of Policies}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ivanov, Dmitry and Ben-Porat, Omer}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the AAAI Conference on Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="wang2023deep" class="col-sm-10"> <div class="title">Deep Contract Design via Discontinuous Networks</div> <div class="author"> <a href="https://tonghanwang.github.io/" rel="external nofollow noopener" target="_blank">Tonghan Wang</a> ,¬† <a href="https://paulduetting.com/" rel="external nofollow noopener" target="_blank">Paul Duetting</a> ,¬† <em>Dmitry Ivanov</em> ,¬† <a href="http://inbaltalgam.com/" rel="external nofollow noopener" target="_blank">Inbal Talgam-Cohen</a> ,¬† and¬† <a href="https://parkes.seas.harvard.edu/" rel="external nofollow noopener" target="_blank">David C Parkes</a> </div> <div class="periodical"> <em>In Thirty-seventh Conference on Neural Information Processing Systems</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2307.02318" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://neurips.cc/virtual/2023/poster/70374" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </div> <div class="abstract hidden"> <p>Contract design involves a principal who establishes contractual agreements about payments for outcomes that arise from the actions of an agent. In this paper, we initiate the study of deep learning for the automated design of optimal contracts. We introduce a novel representation: the Discontinuous ReLU (DeLU) network, which models the principal‚Äôs utility as a discontinuous piecewise affine function of the design of a contract where each piece corresponds to the agent taking a particular action. DeLU networks implicitly learn closed-form expressions for the incentive compatibility constraints of the agent and the utility maximization objective of the principal, and support parallel inference on each piece through linear programming or interior-point methods that solve for optimal contracts. We provide empirical results that demonstrate success in approximating the principal‚Äôs utility function with a small number of training samples and scaling to find approximately optimal contracts on problems with a large number of actions and outcomes.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wang2023deep</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep Contract Design via Discontinuous Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Tonghan and Duetting, Paul and Ivanov, Dmitry and Talgam-Cohen, Inbal and Parkes, David C}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Thirty-seventh Conference on Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="ivanov2023mediated" class="col-sm-10"> <div class="title">Mediated Multi-Agent Reinforcement Learning</div> <div class="author"> <em>Dmitry Ivanov</em> ,¬† Ilya Zisman ,¬† and¬† Kirill Chernyshev </div> <div class="periodical"> <em>In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2306.08419" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/abs/10.5555/3545946.3598618" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>The majority of Multi-Agent Reinforcement Learning (MARL) literature equates the cooperation of self-interested agents in mixed environments to the problem of social welfare maximization, allowing agents to arbitrarily share rewards and private information. This results in agents that forgo their individual goals in favour of social good, which can potentially be exploited by selfish defectors. We argue that cooperation also requires agents‚Äô identities and boundaries to be respected by making sure that the emergent behaviour is an equilibrium, i.e., a convention that no agent can deviate from and receive higher individual payoffs. Inspired by advances in mechanism design, we propose to solve the problem of cooperation, defined as finding socially beneficial equilibrium, by using mediators. A mediator is a benevolent entity that may act on behalf of agents, but only for the agents that agree to it. We show how a mediator can be trained alongside agents with policy gradient to maximize social welfare subject to constraints that encourage agents to cooperate through the mediator. Our experiments in matrix and iterative games highlight the potential power of applying mediators in MARL.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ivanov2023mediated</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Mediated Multi-Agent Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ivanov, Dmitry and Zisman, Ilya and Chernyshev, Kirill}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{49--57}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="ivanov2022optimal" class="col-sm-10"> <div class="title">Optimal-er auctions through attention</div> <div class="author"> <em>Dmitry Ivanov</em> ,¬† Iskander Safiulin ,¬† Igor Filippov ,¬† and¬† Ksenia Balabaeva </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2202.13110" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://papers.nips.cc/paper_files/paper/2022/hash/e0c07bb70721255482020afca44cabf2-Abstract-Conference.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>RegretNet is a recent breakthrough in the automated design of revenue-maximizing auctions. It combines the flexibility of deep learning with the regret-based approach to relax the Incentive Compatibility (IC) constraint (that participants prefer to bid truthfully) in order to approximate optimal auctions. We propose two independent improvements of RegretNet. The first is a neural architecture denoted as RegretFormer that is based on attention layers. The second is a loss function that requires explicit specification of an acceptable IC violation denoted as regret budget. We investigate both modifications in an extensive experimental study that includes settings with constant and inconstant numbers of items and participants, as well as novel validation procedures tailored to regret-based approaches. We find that RegretFormer consistently outperforms RegretNet in revenue (ie is optimal-er) and that our loss function both simplifies hyperparameter tuning and allows to unambiguously control the revenue-regret trade-off by selecting the regret budget.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ivanov2022optimal</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Optimal-er auctions through attention}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ivanov, Dmitry and Safiulin, Iskander and Filippov, Igor and Balabaeva, Ksenia}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{35}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{34734--34747}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="text-additional"> <h2 style="color: inherit">Reviewer Activities</h2> <ul> <li>Program Chair at EC 2024 ‚Äì The 25th ACM Conference on Economics and Computation</li> <li>Reviewer at ICML 2024 ‚Äì The 41st International Conference on Machine Learning</li> <li>Reviewer at ICLR 2024 ‚Äì The 12th International Conference on Learning Representations</li> <li>Reviewer at NeurIPS 2023 ‚Äì The 37th Annual Conference on Neural Information Processing Systems</li> <li>Reviewer at ICML 2023 ‚Äì The 40th International Conference on Machine Learning</li> <li>Reviewer at NeurIPS 2022 ‚Äì The 36th Annual Conference on Neural Information Processing Systems</li> <li>Subreviewer at OPTIMA 2021 ‚Äì The XII International Conference ‚ÄúOptimization and Applications‚Äù</li> <li>Subreviewer at LOD 2020 ‚Äì The 6th International Conference on Machine Learning, Optimization, and Data Science</li> </ul> <h2 style="color: inherit">Personal Statement</h2> <p>On February 24th, 2022, Russian forces invaded Ukraine. It took me a while to even realize what had happened and what the consequences might be. But a week later, I left my hometown of St. Petersburg with my wife out of concern about getting drafted to fight in an unjust, cruel, and unnecessary war.</p> <p>We went to visit my wife‚Äôs parents in Kazakhstan ‚Äì a `short vacation‚Äô as we declared to border control. The decision was made spontaneously. As we woke up one day, we found and bought flight tickets for the evening, having only few hours to pack. This is not an uncommon story.</p> <p>The company I was working for, JetBrains, openly condemned the war and made plans to leave the country in a month. I was offered a relocation package but declined.</p> <p>Few months went by. Feeling no immediate threat, we returned to St. Petersburg. Life was eerily unchanged, apart from vomit-inducing banners with a letter `Z‚Äô.</p> <p>I was fortunate to get a postdoctoral position at Technion in Haifa, Israel. My starting date and the day we relocated was September 20th, which is also my birthday. On September 21st, Russia began a massive military recruitment, causing a second mass relocation wave. We dodged this bullet by one day. I still haven‚Äôt been back.</p> <p>In Israel, I was hoping to heal mentally and focus on professional development. One can imagine how that went.</p> <p>Again, it took us about a week to decide to go on a short vacation ‚Äì this time to Istanbul. We came back in only a few weeks ‚Äì perhaps being more experienced. Despite the still looming threat of war escalation and acts of terror, the prospects of returning to Russia even temporarily seem as dangerous, so we are staying put.</p> <p>However difficult, I try to stay positive and focus on advancing my academic career. Turning down relocation in favour of the uncertainty in the early days of war was not easy, but I was confident in my ability to find a better position that allows me to continue research. Which I did. To my ability, I make an effort to meet new people and engage in new projects. I am proud of the research I‚Äôve contributed to in the past few years.</p> <p>I want to deeply and sincerely thank all my friends who have and continue to care, support, and help. Fedor Sandomirsky who proactively referred me to his former colleagues in Technion. Herve Moulin who wrote me a recommendation letter despite us not working on any joint projects. My PhD advisor, Alexander Nesterov, who always offered me advice and a place at the Game Theory lab in HSE University. My Israeli colleagues, Omer Ben-Porat and Inbal Talgam-Cohen, who expressed compassion and eagerness to help despite being incomparably more affected by the despicable events of October 7th. My international colleagues, David Parkes, Paul Duetting, and Tonghan Wang, who offered sympathy as events were unfolding. My wife Xenia who is staying by my side. And our cat Beba who is a consistent source of dopamine and scratches.</p> <p><img src="/assets/img/beba.jpg" alt="" style="display: block; margin: auto; width: 33%;"></p> <p>Hoping for a better future,</p> <p>Dima</p> <p>20.02.2024</p> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%64%69%76%61%6E%6F%76.%6D%6C@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://telegram.me/dimonenka" title="telegram" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-telegram"></i></a> <a href="https://scholar.google.com/citations?user=G9szMAwAAAA" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/dimonenka" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> </div> <div class="contact-note"></div> </div> </article> </div> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> </body> </html>